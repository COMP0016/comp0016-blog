<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="" />
    <meta name="author" content="Team 38" />
    <title>Implementation | WAPETS 2.0</title>

    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3"
      crossorigin="anonymous"
    />

    <style>
      html,
      body {
        overflow-x: hidden; /* Prevent scroll on narrow devices */
      }

      @media (max-width: 991.98px) {
        .offcanvas-collapse {
          position: fixed;
          top: 56px; /* Height of navbar */
          bottom: 0;
          left: 100%;
          width: 100%;
          padding-right: 1rem;
          padding-left: 1rem;
          overflow-y: auto;
          visibility: hidden;
          background-color: #343a40;
          transition: transform 0.3s ease-in-out, visibility 0.3s ease-in-out;
        }
        .offcanvas-collapse.open {
          visibility: visible;
          transform: translateX(-100%);
        }
      }
    </style>
  </head>
  <body>
    <nav
      class="navbar navbar-expand-lg fixed-top navbar-dark bg-dark"
      aria-label="Main navigation"
    >
      <div class="container-fluid">
        <a class="navbar-brand" href="index.html" aria-current="page"
          >WAPETS 2.0</a
        >
        <button
          class="navbar-toggler p-0 border-0"
          type="button"
          id="navbarSideCollapse"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>

        <div class="navbar-collapse offcanvas-collapse">
          <ul class="navbar-nav me-auto mb-2 mb-lg-0">
            <li class="nav-item">
              <a class="nav-link active" href="01-requirements.html"
                >Requirements</a
              >
            </li>
            <li class="nav-item">
              <a class="nav-link active" href="02-hci.html">HCI</a>
            </li>
            <li class="nav-item">
              <a class="nav-link active" href="03-research.html">Research</a>
            </li>
            <li class="nav-item">
              <a class="nav-link active" href="04-system-design.html"
                >System design</a
              >
            </li>
            <li class="nav-item">
              <a class="nav-link active" href="05-implementation.html"
                >Implementation</a
              >
            </li>
            <li class="nav-item">
              <a class="nav-link active" href="06-testing.html">Testing</a>
            </li>
            <li class="nav-item">
              <a class="nav-link active" href="07-evaluation.html"
                >Evaluation</a
              >
            </li>
            <li class="nav-item">
              <a class="nav-link active" href="08-appendix.html">Appendix</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <main class="container pt-5">
      <div class="row">
        <div class="col-md-8 offset-md-2">
          <h2 class="mt-3">Mobile App Implementation</h2>
          <div class="card mb-3">
            <div class="card-body">
              <h3 class="mt-3">AR Environment</h3>
              <p class="card-text">
                The app is written in C# on the Unity Android and iOS platform.
                It uses the popular ARCore and ARKit libraries for each platform
                respectively to interact with the camera environment on display.
              </p>
              <p class="card-text">
                The scene makes use of AR planes, both vertical and horizontal
                to map out the environment in the AR Viewport. Each plane is
                then judged on its walkability and enabled for ray-casting
              </p>
              <h3 class="mt-3">The Pet</h3>
              <p class="card-text">
                The app displays an anthropomorphic pet that can walk around the
                planes mapped by the AR Plane Manager object.
              </p>
              <p class="card-text">
                The Pet game object is composed of an Animator (in charge of
                controlling skeletal poses), a mesh/avatar (the 3D skinned mesh
                on display) and a Behaviour Script that allows the pet to walk
                around the AR viewport when in input is presented.
                <img
                  class="img img-fluid mb-3"
                  src="./assets/img/implementation/code_snippets/ARPetRaycast.png"
                />
              </p>
              <p class="card-text">
                In addition to its input driven movement, the pet also tracks
                the user's location by turning to always face the camera.
              </p>
              <h3 class="mt-3">Voice Command Functionality</h3>
              <p class="card-text">
                Much like on the desktop version, the pet actively listens for
                voice commands using the phones microphone. Once a sentence is
                recorded, it is then transcribed into text using the IBM Cloud
                SDK's Speech-To-Text functionality. The text is scanned for
                available commands, a reply is selected, and an appropriate
                audio clip is generated by the SDK's Text-To-Speech module.
              </p>
              <h3 class="mt-3">The UI</h3>
              <p class="card-text">
                The user-interface is implemented as a collection of canvases
                and a UI manager. Each of the UI canvases has a main panel and a
                back button. There are canvases for the pets belonging to the
                user, settings, user information and accessories if available
                for the current pet.
                <img
                  class="img img-fluid mb-3"
                  src="./assets/img/implementation/code_snippets/MobileUI.png"
                />
              </p>
              <h3 class="mt-3">Web API (C#)</h3>
              <p class="card-text">
                The API uses a .NET HttpClient to send and receive requests from
                the web server.
                <img
                  class="img img-fluid mb-3"
                  src="./assets/img/implementation/code_snippets/WebAPICode.png"
                />
              </p>
              <p class="card-text">
                Each request and response are formatted from JSON strings to C#
                structs and vice versa using Newtonsoft.Json. Requests are sent
                asynchronously, and responses are received asynchronously.
              </p>
              <p class="card-text">
                Each response is serialized into an APIResponse&lt;T&gt; generic
                structure that contains a T type payload and error code if the
                request was unsuccessful.
                <img
                  class="img img-fluid mb-3"
                  src="./assets/img/implementation/code_snippets/WebAPIResponse.png"
                />
              </p>
              <h3 class="mt-3">Functionality</h3>
              <p class="card-text">
                The web api has commands for all the functionality available
                from the Web Server.
              </p>

              <p class="card-text">
                It has both regular and Async versions of each request.
              </p>
              <p class="card-text">
                The api stores session information into a Session object which
                is then used to request data from the server..
              </p>
              <h3 class="mt-3">Documentation</h3>
              <p class="card-text">
                Documentation of the API is available on its GitHub repository
                README.
              </p>
              <h3 class="mt-3">Testing tools (Developer CLI)</h3>
              <p class="card-text">
                Together with the API is also packaged a command line tool that
                support executing commands on the web server directly
              </p>
              <p class="card-text">
                It was mainly used for managing pet data during development
                sessions
                <img
                  class="img img-fluid mb-3"
                  src="./assets/img/implementation/code_snippets/TestingCLI.png"
                />
              </p>
            </div>
          </div>
          <hr />

          <h2 class="mt-3">Desktop Assistant Implementation</h2>
          <div class="card mb-3">
            <div class="card-body">
              <h3 class="mt-3">Login</h3>
              <p class="card-text">
                The login screen sends and receives data from the web server
                when the user enters their login credentials. The login fails if
                the post request does not contain the user login information;
                otherwise, the user account information is verified and the
                SceneManager.LoadScene() method is used to load the
                application/GUI.
                <img
                  class="img img-fluid mb-3"
                  src="./assets/img/implementation/code_snippets/login.png"
                />
              </p>
              <h3 class="mt-3">Speech-to-text input</h3>
              <p class="card-text">
                The speech-to-text input works by the user’s microphone
                detecting the user’s speech input. This input is then sent by
                the STT (Speech to Text) manager to the Watson backend and
                retrieves the text string in response. The Assistant script then
                checks if the command is spoken and sends the text string to the
                chatbot. After receiving a response from the chatbot, the speech
                bubble shows up next to the pet. Meanwhile, the TTS (Text to
                Speech) sends the response text to the Watson backend, gets the
                audio clip in response to the user’s input, then plays the
                audio.
              </p>
              <h3 class="mt-3">Chatbot</h3>
              <p class="card-text">
                The chatbot is implemented using an open-source machine learning
                framework for automated text and voice-based communications
                software called Rasa. Rasa is a highly customisable,
                Python-based software that enables our project to interpret the
                input provided by the user in the form of speech-to-text and
                respond with the appropriate reply.
              </p>
              <h3 class="mt-3">Voice Control</h3>
              <p class="card-text">
                We implemented 2 types of voice commands in Unity.
              </p>
              <p class="card-text">
                The first are built-in Unity functions, like “play music” and
                “start timer”.
              </p>

              <p class="card-text">
                The second are external commands like “Open BBC”, “Search how to
                run”, etc. These commands can be customised by editing the
                configuration file under resources/configs folder. The app reads
                these commands from the config file and stores the commands in a
                dictionary.
              </p>
              <p class="card-text">
                Both methods use keyword detection. If the keyword is detected
                (after it is processed by IBM’s Watson Cloud SDK Speech-to-text
                function), the corresponding command is executed.
              </p>
              <h3 class="mt-3">Desktop Gadgets</h3>
              <p class="card-text">
                The desktop widgets include a music player with an autoscrolling
                bar, that is updated every frame by changing the x-y positions
                of the related GUI elements during each frame. The timer
                functions by calculating the time between the clicks on the
                start and stop buttons, while the real-time clock displays the
                time using Unity's Quaternion.Euler() methods to rotate the hour
                and minute hands.
                <img
                  class="img img-fluid mb-3"
                  src="./assets/img/implementation/code_snippets/auto_scroll.png"
                />
              </p>
              <h3 class="mt-3">Desktop Pet</h3>
              <p class="card-text">
                Our first implementation of the desktop pet was to have it look
                at the cursor. However, we scrapped this idea as the pet looking
                at the cursor was too weird and unnatural. We also tried
                changing the background as time-of-day changes – like stars
                during night, but we decided against it as its functionality was
                buggy and could cause some problems for people in different time
                zones. In our current implementation, the pet AI is essentially
                a state machine that reacts to different clicks. The OnDrag()
                method allows for the user to drag the pet across the screen,
                having the pet follow the mouse’s x-y coordinates on the screen
                plane to update the location of the virtual pet. The
                SwitchAnimation() method under the ReactToInteraction.cs script
                detects the input of the users mouse (left click, right click,
                scroll click or drag) to check whether the mouse is on the pet
                when it is clicked; if it is, then it redirectst the code to the
                appropriate pet interaction method and causes the pet to react
                accordingly.
                <img
                  class="img img-fluid mb-3"
                  src="./assets/img/implementation/code_snippets/clear_signal.png"
                />
              </p>
              <h3 class="mt-3">Some notes on Design Patterns</h3>
              <p class="card-text">
                We used the Singleton pattern for the WatsonManager and
                GameManager because we want them to be globally accessible and
                to be instantiated only once – they will not be destroyed upon
                entering the AR mode.
                <img
                  class="img img-fluid mb-3"
                  src="./assets/img/implementation/code_snippets/singleton.png"
                />
              </p>
              <p class="card-text">
                We used the Observer pattern for the SpeechBubble and
                TextToSpeech. This way, the Assistant sends a notification after
                receiving a response from the chatbot. After observing it, the
                SpeechBubble shows up and starts a timer.
                <img
                  class="img img-fluid mb-3"
                  src="./assets/img/implementation/code_snippets/timer.png"
                />
                When the timer reaches 0, the SpeechBubble disappears.
                TextToSpeech sends the text string to the backend, retrieves an
                audio clip, and plays it.
                <img
                  class="img img-fluid mb-3"
                  src="./assets/img/implementation/code_snippets/observer.png"
                />
                <img
                  class="img img-fluid mb-3"
                  src="./assets/img/implementation/code_snippets/observer2.png"
                />
              </p>
            </div>
          </div>
          <hr />

          <h2 class="mt-3">Web Server</h2>
          <div class="card mt-3 mb-3">
            <div class="card-body">
              <h3 class="mt-3">Hosting</h3>
              <p class="card-text">
                The website is hosted on DigitalOcean App Platform, due to its
                speed of setup and continuous integration ability
              </p>
              <p class="card-text">
                Every time a commit is pushed to the master branch of the git
                repository, the website is redeployed (with automatic rollback
                if there is an issue with the deploy)
              </p>
              <p class="card-text">
                There is also a PostgreSQL database managed by DigitalOcean
              </p>
              <h3 class="mt-3">Web framework</h3>
              <p class="card-text">
                The code for the web server is written in Python, using the
                Django web framework
              </p>
              <p class="card-text">
                This is due to the range of features included in Django
                (including the ORM, and automated admin panel) as well as the
                wealth of libraries available for both Python and Django which
                make developing common patterns much easier
              </p>
              <p class="card-text">
                The ORM is used to manage the tables that store users and pets,
                and stably migrate database state when changes are made
              </p>
              <h3 class="mt-3">API</h3>
              <p class="card-text">
                The API is written using <b>djangorestframework</b> which sits
                on top of Django’s view system, to provide a set of common
                classes to do common API operations
              </p>
              <p class="card-text">
                The <b>ModelViewSet</b> provides the ability to create, read,
                list, update, and delete records in a database table (with the
                ability to add custom filters, for example the
                <b>PetViewSet</b> only shows pets for the current user)
              </p>
              <p class="card-text">
                The <b>Router</b> then links various URLS (/api/pet/3/update
                etc.) to their relevant views within the <b>ModelViewSet</b>
              </p>
              <p class="card-text">
                There is a <b>PetViewSet</b> which provides full CRUD for the
                <b>Pet</b> model
                <img
                  class="img img-fluid mb-3"
                  src="./assets/img/implementation/code_snippets/PetViewSet.png"
                />
              </p>
              <p class="card-text">
                There is <b>UserViewSet</b> which just provides an endpoint for
                the currently logged in user to get information about their
                account (name, preferences etc.)
              </p>
              <h3 class="mt-3">Authentication</h3>
              <p class="card-text">
                Uses JSON Web Tokens (JWT) which is an open standard for sending
                encryption and signatures as JSON
              </p>
              <p class="card-text">
                A user sends a username and password to an API endpoint and, if
                they match a user account in the system, they are returned a JWT
              </p>
              <p class="card-text">
                This JWT contains which account the user is authenticated to,
                along with the expiry date (1 week after obtaining), and a
                signature generated using a key stored on the server which
                verifies the enclosed information
              </p>
              <p class="card-text">
                This token should be included with every API request as a bearer
                token in the Authorization header.
              </p>
              <p class="card-text">
                This allows the server to know that the user is authenticated to
                access the API, and specifically which account they are
                authenticated to (so that the correct data can be returned)
              </p>
              <p class="card-text">
                The underlying implementation of this signing algorithm is done
                by a python library call <b>drf-jwt</b>
              </p>
              <h3 class="mt-3">Documentation</h3>
              <p class="card-text">
                There is API documentation provided as a hosted webpage,
                alongside the API
              </p>
              <p class="card-text">
                This is achieved using a few pieces of open-source technology
                connected together
              </p>
              <p class="card-text">
                <b>Swagger</b> is a system to describe API’s using JSON, in
                order to make them indexable by automated systems
              </p>
              <p class="card-text">
                There is a related project called <b>Swagger UI</b> which can
                take an API description in Swagger and create very detailed API
                documentation, including example requests and responses
              </p>
              <p class="card-text">
                Lastly, there is a library called <b>drf-yasg</b> which
                generates a Swagger description of a djangorestframework API
              </p>
              <p class="card-text">
                Combining drf-yasg and Swagger UI, we were able to generate very
                usable API documentation, with all of the definitions coming
                from the same code that actually implements the API. There was
                some amount of difficulty changing the default functionality of
                both drf-yasg and Swagger UI, to customise the documentation a
                bit, but this is to be expected.
              </p>
            </div>
          </div>
          <hr />

          <h2 class="mt-3">HCI</h2>
          <div class="card mt-3 mb-3">
            <div class="card-body">
              <p class="card-text">
                As for interaction, we use UCL’s Motion Input V2. Recent
                advancements in gesture recognition technology, computer vision
                and machine learning open up a world of new opportunities for
                touchless computing interactions. UCL’s MotionInput supporting
                DirectX v2.0 is the second iteration of our Windows based
                library. This uses several open-source and federated on-device
                machine learning models, meaning that it is privacy safe in
                recognising users. It captures and analyses interactions and
                converts them into mouse and keyboard signals for the operating
                system to make use of in its native user interface [5].
              </p>
            </div>
          </div>
          <hr />

          <h2 class="mt-3">AR</h2>
          <div class="card mt-3 mb-3">
            <div class="card-body">
              <p class="card-text">
                The main AI technology used is Automated Speech Recognition,
                text to speech and recommendation systems.
              </p>
              <p class="card-text">
                IBM Watson: Watson is IBM’s suite of enterprise-ready Artificial
                Intelligence (AI) services, applications, and tooling [6].
                Automated Speech Recognition and Text to Speech (TTS) are
                integrated within IBM Watson.
              </p>
              <p class="card-text">
                Automated Speech Recognition (ASR): IBM Watson® Speech to Text
                technology enables fast and accurate speech transcription in
                multiple languages for a variety of use cases. The service
                leverages machine learning to combine knowledge of grammar,
                language structure, and the composition of audio and voice
                signals to accurately transcribe the human voice.
              </p>
              <p class="card-text">
                Text to speech (TTS): IBM Watson® Text to Speech is an API cloud
                service that enables user to convert written text into
                natural-sounding audio in a variety of languages and voices
                within an existing application or within Watson Assistant. The
                audio uses appropriate cadence and intonation for its language
                and dialect to provide voices that are smooth and natural.
              </p>
              <p class="card-text">
                Recommender system: A recommender system, or a recommendation
                system (sometimes replacing 'system' with a synonym such as
                platform or engine), is a subclass of information filtering
                system that seeks to predict the "rating" or "preference" a user
                would give to an item [7]. One approach to the design of
                recommender systems that has wide use is collaborative
                filtering. Collaborative filtering is based on the assumption
                that people who agreed in the past will agree in the future, and
                that they will like similar kinds of items as they liked in the
                past. The system generates recommendations using only
                information about rating profiles for different users or items.
                By locating peer users/items with a rating history similar to
                the current user or item, they generate recommendations using
                this neighborhood.
              </p>
            </div>
          </div>
          <hr />

          <h2 class="mt-3">Animations</h2>
          <div class="card mt-3 mb-3">
            <div class="card-body">
              <p class="card-text">
                Characters, animals, plants, and other moving objects in the
                game need to be designed by game animators to achieve realistic
                effects. Generally, 3ds Max, After Effects, and Photoshop are
                used to do bone skin binding production. However, due to
                shortness of time and people, we may not have the time to learn
                and do these from scratch. Website like Mixamo, Free3d provides
                free 3d models and animations, which could be of great help.
              </p>
            </div>
          </div>
          <hr />

          <h2 class="mt-3">Server</h2>
          <div class="card mt-3 mb-3">
            <div class="card-body">
              <p class="card-text">
                Since our project is multi-platform, we’ll need a server to
                ensure devices of the same account see identical behavior of a
                pet, so that scenes like a cat jump from one screen to another
                is achievable. Pets’ data should also be shared among devices
                with no differences.
              </p>
              <p class="card-text">
                Game server: Our server needs to synchronize the position and
                status of the pet in the current scene to each client as the
                minimum standard. This form is a bit like a Battle.net server.
                The server will choose one person to be the Host, and others
                will be P2P to connect to the player who is in charge. STUN is
                to help players establish a P2P traction serve r, and because
                P2P connectivity is only about 75%, players who are unconnected
                will forward it through Forward. [Figure 1] A weak interaction
                HTTP server should also be sufficient. [Figure 2] It only needs
                a browser to make debugging clear.
              </p>
              <div>
                <table style="margin: auto">
                  <tr>
                    <td>
                      <img
                        class="img img-fluid mb-3"
                        src="./assets/img/research/fg1.png"
                      />
                    </td>
                    <td>
                      <img
                        class="img img-fluid mb-3"
                        src="./assets/img/research/fg2.png"
                      />
                    </td>
                  </tr>
                </table>
              </div>
              <p class="card-text">
                Cloud server: More and more people choose cloud servers, which
                can provide better security and stability, and reduce the
                difficulty of development, operation, and maintenance. Google
                cloud may be a good start.
              </p>
            </div>
          </div>
          <hr />

          <h2 class="mt-3">Review of Similar Projects</h2>
          <div class="card mt-3 mb-3">
            <div class="card-body">
              <p class="card-text">
                Pokémon Go is one of the most famous and earliest AR games, and
                it has similarities with AR part of our project. In Pokémon Go,
                users catch and collect Pokémon under AR mode. It encourages
                users to go out by integrating Google Map API within, so that
                the more places user discovered, the more Pokémon he will
                collect, the more powerful he will be. They can train it by
                having combat with other trainers, interacting with them or
                feeding them to increase intimacy. Likewise, in WAPETS2.0 users
                can interact, play and feed their WAPETs under AR mode, and
                there will be intimacy system and social media integration. Many
                researchers suggests that Pokémon GO reduced anxiety about
                interacting with strangers [8], Pokémon GO increases
                attention-concentration in adolescents and players feel more
                happy than non-players [9], and help to build social
                connectedness through real-world engagement [10]. We have reason
                to hope that WAPETS2.0 can bring such a positive impact to
                society as well.
              </p>
              <div class="col-xl-8 offset-xl-2">
                <img
                  class="img img-fluid mb-3"
                  src="./assets/img/research/pokemon_go.jpg"
                />
              </div>
              <p class="card-text">
                For Desktop App, Desktop Goose [11] may be the latest most
                similar project. It has a large number of interactions - nab
                user's mouse, track mud on screen, leave user a message, deliver
                user memes... However, we would expect our desktop pet to be
                more serious - sit idly, and only move on drag. In addition, we
                will have functionalities of voice control and chatbot.
              </p>
            </div>
          </div>
          <hr />

          <h2 class="mt-3">References</h2>
          <div class="card mt-3 mb-3">
            <div class="card-body">
              <ol>
                <li>
                  Technologies, U., 2022. Unity Platform. [online] Unity.
                  Available at: https://unity.com/products/unity-platform
                  [Accessed 20 February 2022].
                </li>
                <li>
                  Innovecs Games. 2022. Unity for XR Development: Why is it the
                  Best Choice So Far?. [online] Available at:
                  https://www.innovecsgames.com/blog/unity-for-xr-development/
                  [Accessed 20 February 2022].
                </li>
                <li>
                  Docs.microsoft.com. 2022. C# docs. [online] Available at:
                  https://docs.microsoft.com/en-us/dotnet/csharp/ [Accessed 20
                  February 2022].
                </li>
                <li>
                  3D Development Bootcamp & XR Courses | Circuit Stream. 2022.
                  Ultimate AR Comparison Guide | Circuit Stream. [online]
                  Available at:
                  https://circuitstream.com/blog/augmented-reality-guide/
                  [Accessed 20 February 2022].
                </li>
                <li>
                  UCL Computer Science. 2022. Touchless Computing with UCL’s
                  MotionInput v2.0 and Microsoft. [online] Available at:
                  https://www.ucl.ac.uk/computer-science/news/2021/jul/touchless-computing-ucls-motioninput-v20-and-microsoft
                  [Accessed 20 February 2022].
                </li>
                <li>
                  Ibm.com. 2022. IBM Academic Initiative. [online] Available at:
                  https://www.ibm.com/academic/topic/artificial-intelligence
                  [Accessed 20 February 2022].
                </li>
                <li>
                  Francesco Ricci and Lior Rokach and Bracha Shapira,
                  Introduction to Recommender Systems Handbook, Recommender
                  Systems Handbook, Springer, 2011, pp. 1-35
                </li>
                <li>
                  Lori Kogan, Peter Hellyer, Colleen Duncan, Regina
                  Schoenfeld-Tacher, A pilot investigation of the physical and
                  psychological benefits of playing Pokémon GO for dog owners,
                  Computers in Human Behavior, Volume 76, 2017, Pages 431-437,
                  ISSN 0747-5632, https://doi.org/10.1016/j.chb.2017.07.043.
                  (https://www.sciencedirect.com/science/article/pii/S0747563217304661)
                </li>
                <li>
                  Alberto Ruiz-Ariza, Rafael Antonio Casuso, Sara
                  Suarez-Manzano, Emilio J. Martínez-López, Effect of augmented
                  reality game Pokémon GO on cognitive performance and emotional
                  intelligence in adolescent young, Computers & Education,
                  Volume 116, 2018, Pages 49-63, ISSN 0360-1315,
                  https://doi.org/10.1016/j.compedu.2017.09.002.
                  (https://www.sciencedirect.com/science/article/pii/S0360131517302002)
                </li>
                <li>
                  Vella K, Johnson D, Cheng VWS, et al. A Sense of Belonging:
                  Pokémon GO and Social Connectedness. Games and Culture.
                  2019;14(6):583-603. doi:10.1177/1555412017719973
                </li>
                <li>
                  Goose, D., 2022. Desktop Goose by samperson. [online] itch.io.
                  Available at: https://samperson.itch.io/desktop-goose
                  [Accessed 29 March 2022].
                </li>
              </ol>
            </div>
          </div>
        </div>
      </div>

      <footer
        class="d-flex flex-wrap justify-content-between align-items-center py-3 my-4 border-top"
      >
        <p class="col-md-4 mb-0 text-muted">
          &copy;
          <script>
            document.write(new Date().getFullYear());
          </script>
          Team 38
        </p>

        <!--        <a href="/2021/group38" class="col-md-4 d-flex align-items-center justify-content-center mb-3 mb-md-0 me-md-auto link-dark text-decoration-none">-->
        <!--          <svg class="bi me-2" width="40" height="32"><use xlink:href="#bootstrap"/></svg>-->
        <!--        </a>-->

        <ul class="nav col-md-4 justify-content-end">
          <li class="nav-item">
            <a href="/2021/group38" class="nav-link px-2 text-muted">Home</a>
          </li>
        </ul>
      </footer>
    </main>

    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
      crossorigin="anonymous"
    ></script>

    <script>
      (function () {
        "use strict";

        document
          .querySelector("#navbarSideCollapse")
          .addEventListener("click", function () {
            document
              .querySelector(".offcanvas-collapse")
              .classList.toggle("open");
          });
      })();
    </script>
  </body>
</html>
